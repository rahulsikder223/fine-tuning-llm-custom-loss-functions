{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11484,
     "status": "ok",
     "timestamp": 1733426891674,
     "user": {
      "displayName": "Jianguo Lu",
      "userId": "17051011119190322880"
     },
     "user_tz": 300
    },
    "id": "Ny3Ckh888coX",
    "outputId": "00463ee5-8fe3-42f9-d582-ce33cb3fd579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDkFdtln8cof"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O789b8r38cog"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yopQ8Rs8coj"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFodqLWJ8cok"
   },
   "outputs": [],
   "source": [
    "def get_senteval_dataset(dataset_name):\n",
    "    dataset = load_dataset(f'rahulsikder223/SentEval-{dataset_name}')\n",
    "    concatenated_dataset = concatenate_datasets([dataset['train'], dataset['test']])\n",
    "    concatenated_dataset = concatenated_dataset.rename_column(\"label\", \"labels\")\n",
    "    return concatenated_dataset\n",
    "\n",
    "senteval_datasets = ['CR', 'MPQA', 'MR', 'SUBJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGlQVXDP8com"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bO2nloZ8con"
   },
   "outputs": [],
   "source": [
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDVx0vu58coq"
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLYTWsiv8coq"
   },
   "source": [
    "## Classification Losses - Single sentence column and sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyH9-7_A8cos"
   },
   "source": [
    "### Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyYvypHC8cot"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_Ks8ugG8cot"
   },
   "source": [
    "### Label Smoothing Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTglwbzF8cou"
   },
   "outputs": [],
   "source": [
    "def label_smoothing_cross_entropy_loss(logits, labels, smoothing=0.1):\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Initializing true distribution with smoothing value for all classes...\n",
    "    true_dist = torch.full_like(log_probs, smoothing / (log_probs.size(1) - 1))\n",
    "    # Setting the true label confidence in the correct class...\n",
    "    true_dist.scatter_(1, labels.unsqueeze(1), confidence)\n",
    "\n",
    "    loss = torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Menku38cov"
   },
   "source": [
    "## Embedding Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC_u0wnq8cow"
   },
   "source": [
    "### Triplets - Anchor, Positive and Negative sentences (3 sentence columns) and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTGEqlMg8cow"
   },
   "source": [
    "#### Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8AOSZU78cox"
   },
   "outputs": [],
   "source": [
    "def generate_triplets(embeddings, labels):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for i, anchor_label in enumerate(labels):\n",
    "        anchor = embeddings[i]\n",
    "\n",
    "        # Finding a positive example (same label as anchor)...\n",
    "        positive_indices = (labels == anchor_label).nonzero(as_tuple=True)[0].tolist()\n",
    "        # Ensuring positive is not the same as anchor...\n",
    "        positive_indices.remove(i)\n",
    "\n",
    "        # Ensuring there is at least one valid positive example...\n",
    "        if not positive_indices:\n",
    "            continue\n",
    "\n",
    "        # Selecting the first positive (no randomization for now)...\n",
    "        positive_idx = positive_indices[0]\n",
    "        positive = embeddings[positive_idx]\n",
    "\n",
    "        # Finding a negative example (different label)...\n",
    "        negative_indices = (labels != anchor_label).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "        # Ensuring there is at least one valid negative example...\n",
    "        if not negative_indices:\n",
    "            continue\n",
    "\n",
    "        # Select the first negative\n",
    "        negative_idx = negative_indices[0]\n",
    "        negative = embeddings[negative_idx]\n",
    "\n",
    "        # Adding the triplet...\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "\n",
    "    # Ensuring non-empty lists before stacking...\n",
    "    if anchors and positives and negatives:\n",
    "        return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmDPN6P_8coy"
   },
   "outputs": [],
   "source": [
    "def generate_all_triplets(embeddings, labels):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for i, anchor_label in enumerate(labels):\n",
    "        anchor = embeddings[i]\n",
    "\n",
    "        # Find all positive examples (same label as anchor)\n",
    "        positive_indices = (labels == anchor_label).nonzero(as_tuple=True)[0].tolist()\n",
    "        positive_indices.remove(i)  # Exclude the anchor itself\n",
    "\n",
    "        # Find all negative examples (different label from anchor)\n",
    "        negative_indices = (labels != anchor_label).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "        # Generate all valid triplets for this anchor\n",
    "        for pos_idx in positive_indices:\n",
    "            positive = embeddings[pos_idx]\n",
    "            for neg_idx in negative_indices:\n",
    "                negative = embeddings[neg_idx]\n",
    "\n",
    "                # Add the triplet\n",
    "                anchors.append(anchor)\n",
    "                positives.append(positive)\n",
    "                negatives.append(negative)\n",
    "\n",
    "    # Convert lists to tensors for batch processing\n",
    "    if anchors and positives and negatives:\n",
    "        return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0by7rple8coy"
   },
   "outputs": [],
   "source": [
    "def triplet_loss(embeddings, labels, margin=1.0):\n",
    "    anchors, positives, negatives = generate_triplets(embeddings, labels)\n",
    "    if anchors is None or positives is None or negatives is None:\n",
    "        return 0.0\n",
    "\n",
    "    # Euclidean distance between anchor and positive, and anchor and negative...\n",
    "    pos_dist = F.pairwise_distance(anchors, positives)\n",
    "    neg_dist = F.pairwise_distance(anchors, negatives)\n",
    "    loss = torch.clamp(pos_dist - neg_dist + margin, min=0.0)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTTCTawS8coy"
   },
   "outputs": [],
   "source": [
    "def triplet_all_loss(embeddings, labels, margin=1.0):\n",
    "    anchors, positives, negatives = generate_all_triplets(embeddings, labels)\n",
    "    if anchors is None or positives is None or negatives is None:\n",
    "        return 0.0\n",
    "\n",
    "    # Euclidean distance between anchor and positive, and anchor and negative...\n",
    "    pos_dist = F.pairwise_distance(anchors, positives)\n",
    "    neg_dist = F.pairwise_distance(anchors, negatives)\n",
    "    loss = torch.clamp(pos_dist - neg_dist + margin, min=0.0)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3Su9f0u8coz"
   },
   "source": [
    "#### Hard Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NplalhGI8co0"
   },
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Computes the 2D matrix of distances between all the embeddings.\n",
    "    \"\"\"\n",
    "    dot_product = torch.matmul(embeddings, embeddings.t())\n",
    "    square_norm = torch.diag(dot_product)\n",
    "\n",
    "    # Computing the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n",
    "    distances[distances < 0] = 0\n",
    "\n",
    "    if not squared:\n",
    "        mask = distances.eq(0).float()\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = (1.0 -mask) * torch.sqrt(distances)\n",
    "    return distances\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Returns a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    \"\"\"\n",
    "    # Checking that i and j are distinct...\n",
    "    indices_equal = torch.eye(labels.size(0), device=labels.device).bool()\n",
    "    indices_not_equal = ~indices_equal\n",
    "\n",
    "    # Checking if labels[i] == labels[j]...\n",
    "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "\n",
    "    return labels_equal & indices_not_equal\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Returns a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    return ~(labels.unsqueeze(0) == labels.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFy1UYBB8co2"
   },
   "outputs": [],
   "source": [
    "def hard_triplet_loss(embeddings, labels, margin=1.0):\n",
    "    # Calculating pairwise distance matrix - SBERT\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=False)\n",
    "\n",
    "    # Mask to get the hardest positive distances - SBERT\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels).float()\n",
    "    anchor_positive_dist = mask_anchor_positive * pairwise_dist\n",
    "    hardest_positive_dist, _ = anchor_positive_dist.max(dim=1, keepdim=True)\n",
    "\n",
    "    # Mask to get the hardest negative distances - SBERT\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).float()\n",
    "    max_anchor_negative_dist, _ = pairwise_dist.max(dim=1, keepdim=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "    hardest_negative_dist, _ = anchor_negative_dist.min(dim=1, keepdim=True)\n",
    "\n",
    "    tl = hardest_positive_dist - hardest_negative_dist + margin\n",
    "    # Ensuring non-negative loss\n",
    "    tl = F.relu(tl)\n",
    "    triplet_loss = tl.mean()\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqZFidof8co2"
   },
   "source": [
    "### Pairs - 2 sentence columns and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ufj-Zfq8co2"
   },
   "source": [
    "#### Pair Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79aeZf6L8co3"
   },
   "outputs": [],
   "source": [
    "def generate_pairs(embeddings, labels):\n",
    "    embedding1_list = []\n",
    "    embedding2_list = []\n",
    "    similarity_labels = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            embedding1 = embeddings[i]\n",
    "            embedding2 = embeddings[j]\n",
    "\n",
    "            # If the labels are the same, labeling the pair as 1 (similar)...\n",
    "            if labels[i] == labels[j]:\n",
    "                similarity_labels.append(1)\n",
    "            else:\n",
    "                # If the labels are different, labeling the pair as 0 (dissimilar)...\n",
    "                similarity_labels.append(0)\n",
    "\n",
    "            embedding1_list.append(embedding1)\n",
    "            embedding2_list.append(embedding2)\n",
    "\n",
    "    embedding1_tensor = torch.stack(embedding1_list)\n",
    "    embedding2_tensor = torch.stack(embedding2_list)\n",
    "    labels_tensor = torch.tensor(similarity_labels).to(labels.device)\n",
    "\n",
    "    return embedding1_tensor, embedding2_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Myf6jQ8co3"
   },
   "source": [
    "#### Cosine Similarity MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYx8mLhN8co4"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_mse_loss(embeddings, labels):\n",
    "    embedding1, embedding2, labels = generate_pairs(embeddings, labels)\n",
    "\n",
    "    # Calculating the cosine similarity between the pairs of embeddings...\n",
    "    cos_sim = F.cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "    # MSE loss...\n",
    "    squared_difference = (labels - cos_sim) ** 2\n",
    "    loss = squared_difference.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvTDpMrt8co4"
   },
   "source": [
    "#### CoSENT Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q__TfhJQ8co5"
   },
   "outputs": [],
   "source": [
    "def cosent_loss(embeddings, labels, tau=20.0):\n",
    "    embedding1, embedding2, labels = generate_pairs(embeddings, labels)\n",
    "\n",
    "    # Input preparation...\n",
    "    labels = (labels[:, None] < labels[None, :]).float()\n",
    "\n",
    "    # Normalization of Logits...\n",
    "    embedding1 = F.normalize(embedding1, p=2, dim=1)\n",
    "    embedding2 = F.normalize(embedding2, p=2, dim=1)\n",
    "\n",
    "    # Cosine Similarity Calculation...\n",
    "    # The dot product of these pairs gives the cosine similarity, scaled by a factor of tau to control the sharpness of similarity scores...\n",
    "    y_pred = torch.sum(embedding1 * embedding2, dim=1) * tau\n",
    "\n",
    "    # Pairwise cosine similarity difference calculation...\n",
    "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
    "\n",
    "    y_pred = (y_pred - (1 - labels) * 1e12).view(-1)\n",
    "\n",
    "    zero = torch.Tensor([0]).to(y_pred.device)\n",
    "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
    "    return torch.logsumexp(y_pred, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDmd1cI-8co5"
   },
   "source": [
    "#### In-Batch Negatives Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4v-x9rYa8co5"
   },
   "outputs": [],
   "source": [
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return -(F.log_softmax(y_pred, dim=1) * y_true).sum(dim=1)\n",
    "\n",
    "def in_batch_negative_loss(embeddings, labels, tau=20.0, negative_weights=0.0):\n",
    "    device = labels.device\n",
    "    embedding1, embedding2, labels = generate_pairs(embeddings, labels)\n",
    "\n",
    "    y_pred = torch.empty((2 * embedding1.shape[0], embedding1.shape[1]), device=device)\n",
    "    y_pred[0::2] = embedding1\n",
    "    y_pred[1::2] = embedding2\n",
    "    y_true = labels.repeat_interleave(2).unsqueeze(1)\n",
    "\n",
    "    def make_target_matrix(y_true):\n",
    "        idxs = torch.arange(0, y_pred.shape[0]).int().to(device)\n",
    "        y_true = y_true.int()\n",
    "        idxs_1 = idxs[None, :]\n",
    "        idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]\n",
    "\n",
    "        idxs_1 *= y_true.T\n",
    "        idxs_1 += (y_true.T == 0).int() * -2\n",
    "\n",
    "        idxs_2 *= y_true\n",
    "        idxs_2 += (y_true == 0).int() * -1\n",
    "\n",
    "        y_true = (idxs_1 == idxs_2).float()\n",
    "        return y_true\n",
    "\n",
    "    neg_mask = make_target_matrix(y_true == 0)\n",
    "\n",
    "    y_true = make_target_matrix(y_true)\n",
    "\n",
    "    y_pred = F.normalize(y_pred, dim=1, p=2)\n",
    "    similarities = y_pred @ y_pred.T\n",
    "    similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12\n",
    "    similarities = similarities * tau\n",
    "\n",
    "    if negative_weights > 0:\n",
    "        similarities += neg_mask * negative_weights\n",
    "\n",
    "    return categorical_crossentropy(y_true, similarities).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7sqikkh8co6"
   },
   "source": [
    "#### Angle Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrGdbCaL8co6"
   },
   "outputs": [],
   "source": [
    "def angle_loss(embeddings, labels, tau=1.0):\n",
    "    embedding1, embedding2, labels = generate_pairs(embeddings, labels)\n",
    "\n",
    "    # Input preparation...\n",
    "    labels = (labels[:, None] < labels[None, :]).float()\n",
    "\n",
    "    # Chunking into real and imaginary parts...\n",
    "    y_pred_re1, y_pred_im1 = torch.chunk(embedding1, 2, dim=1)\n",
    "    y_pred_re2, y_pred_im2 = torch.chunk(embedding2, 2, dim=1)\n",
    "\n",
    "    a = y_pred_re1\n",
    "    b = y_pred_im1\n",
    "    c = y_pred_re2\n",
    "    d = y_pred_im2\n",
    "\n",
    "    z = torch.sum(c**2 + d**2, dim=1, keepdim=True)\n",
    "    re = (a * c + b * d) / z\n",
    "    im = (b * c - a * d) / z\n",
    "\n",
    "    dz = torch.sum(a**2 + b**2, dim=1, keepdim=True)**0.5\n",
    "    dw = torch.sum(c**2 + d**2, dim=1, keepdim=True)**0.5\n",
    "    re /= (dz / dw)\n",
    "    im /= (dz / dw)\n",
    "\n",
    "    y_pred = torch.concat((re, im), dim=1)\n",
    "    y_pred = torch.abs(torch.sum(y_pred, dim=1)) * tau\n",
    "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
    "    y_pred = (y_pred - (1 - labels) * 1e12).view(-1)\n",
    "    zero = torch.Tensor([0]).to(y_pred.device)\n",
    "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
    "    return torch.logsumexp(y_pred, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZOWltUY8co6"
   },
   "source": [
    "#### Combination of CoSENT, In-Batch Negatives and Angle Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvjeVaJZ8co6"
   },
   "outputs": [],
   "source": [
    "def cosent_ibn_angle(embeddings, labels, w_cosent=1, w_ibn=1, w_angle=1, tau_cosent=20.0, tau_ibn=20.0, tau_angle=1.0):\n",
    "    return w_cosent * cosent_loss(embeddings, labels, tau_cosent) + w_ibn * in_batch_negative_loss(embeddings, labels, tau_ibn) + w_angle * angle_loss(embeddings, labels, tau_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCRCXBqB8co6"
   },
   "source": [
    "## Loss List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBD1ZgGD8co7"
   },
   "outputs": [],
   "source": [
    "losses = [\n",
    "    {'loss_name': 'without_ft', 'loss_type': 'emb', 'loss_kwargs': {}},\n",
    "    {'loss_name': 'cross_entropy_loss', 'loss_type': 'clf', 'loss_kwargs': {}},\n",
    "    {'loss_name': 'label_smoothing_cross_entropy_loss', 'loss_type': 'clf', 'loss_kwargs': {'smoothing': 0.1}},\n",
    "    {'loss_name': 'triplet_loss', 'loss_type': 'emb', 'loss_kwargs': {'margin': 5}},\n",
    "    {'loss_name': 'triplet_all_loss', 'loss_type': 'emb', 'loss_kwargs': {'margin': 5}},\n",
    "    {'loss_name': 'hard_triplet_loss', 'loss_type': 'emb', 'loss_kwargs': {'margin': 5}},\n",
    "    {'loss_name': 'cosine_similarity_mse_loss', 'loss_type': 'emb', 'loss_kwargs': {}},\n",
    "    {'loss_name': 'cosent_loss', 'loss_type': 'emb', 'loss_kwargs': {'tau': 20.0}},\n",
    "    {'loss_name': 'in_batch_negative_loss', 'loss_type': 'emb', 'loss_kwargs': {'tau': 20.0}},\n",
    "    {'loss_name': 'angle_loss', 'loss_type': 'emb', 'loss_kwargs': {'tau': 1.0}},\n",
    "    {'loss_name': 'cosent_ibn_angle', 'loss_type': 'emb', 'loss_kwargs': {'w_cosent': 1, 'w_ibn': 1, 'w_angle': 1, 'tau_cosent': 20.0, 'tau_ibn': 20.0, 'tau_angle': 1.0}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPMwf-lb8co7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKHyDcgz8co7"
   },
   "source": [
    "### Training Preparation (Model Preparation, Embedding Generation, Extraction, Training and Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V2Z4wPt8co7"
   },
   "source": [
    "#### Device Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3Nr2BFY8co7"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqNh7crE8co7"
   },
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4erFwLb8co7"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, split=0.3):\n",
    "    # Dataset Import...\n",
    "    ds = get_senteval_dataset(dataset)\n",
    "\n",
    "    # Random Split...\n",
    "    train_test_split = ds.train_test_split(test_size=split)\n",
    "    train_dataset = train_test_split['train']\n",
    "    test_dataset = train_test_split['test']\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDdvUgrT8co7"
   },
   "source": [
    "#### Model and Tokenizer Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScAyEnEJ8co8"
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer(model_id, loss_type='clf'):\n",
    "    if loss_type == 'clf':\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model.to(device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2WC-rqh8co8"
   },
   "source": [
    "#### Dataset Tokenization and Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH7Hndao8co8"
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset_batch(train_dataset, test_dataset, tokenizer, batch_size):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Torch format setting...\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Batching using DataLoader...\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zATKVsPw8co-"
   },
   "source": [
    "#### Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gv_25Cw8co-"
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(model, device, dataloader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting embeddings\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "            # [CLS] token embeddings...\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(batch['labels'].cpu())\n",
    "\n",
    "    return torch.cat(all_embeddings), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d04FgBbP8co-"
   },
   "source": [
    "#### Train Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8vL8pxe8co-"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_type='clf', epochs=10, loss_name='cross_entropy_loss', **loss_kwargs):\n",
    "    # Optimizer setting...\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Training loop...\n",
    "    num_epochs = epochs\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            if loss_type == 'clf':\n",
    "                # Cross-Entropy Losses...\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                loss = globals()[loss_name](logits, batch['labels'], **loss_kwargs)\n",
    "            else:\n",
    "                # Embedding Loss...\n",
    "                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "                # [CLS] token embedding...\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                loss = globals()[loss_name](embeddings, batch['labels'], **loss_kwargs)\n",
    "                if loss == 0.0:\n",
    "                    continue\n",
    "\n",
    "            # Backpropagation...\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating weights...\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMAPL35V8co-"
   },
   "source": [
    "#### Evaluation Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHc86RP08co-"
   },
   "outputs": [],
   "source": [
    "def evaluate_clf(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass...\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            total_correct += (predictions == batch['labels']).sum().item()\n",
    "            total_samples += batch['labels'].size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3Llqu4p8co-"
   },
   "outputs": [],
   "source": [
    "def evaluate_emb(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Generating embeddings of the train and test sentences...\n",
    "    train_embeddings, train_labels = extract_embeddings(model, device, train_loader)\n",
    "    test_embeddings, test_labels = extract_embeddings(model, device, test_loader)\n",
    "\n",
    "    train_embeddings_np = train_embeddings.numpy()\n",
    "    test_embeddings_np = test_embeddings.numpy()\n",
    "    train_labels_np = train_labels.numpy()\n",
    "    test_labels_np = test_labels.numpy()\n",
    "\n",
    "    # Training a Logistic Regression classifier on the training embeddings...\n",
    "    lr_clf = LogisticRegression(max_iter=10000)\n",
    "    lr_clf.fit(train_embeddings_np, train_labels_np)\n",
    "\n",
    "    # Predicting the labels for the test set...\n",
    "    test_predictions = lr_clf.predict(test_embeddings_np)\n",
    "    accuracy = accuracy_score(test_labels_np, test_predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29co4Auc8co-"
   },
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCpfnAJn8co-"
   },
   "outputs": [],
   "source": [
    "total_runs = 3\n",
    "batch_size = 60\n",
    "accuracy_list = []\n",
    "for loss in losses:\n",
    "    loss_name = loss['loss_name']\n",
    "    loss_type = loss['loss_type']\n",
    "    loss_kwargs = loss['loss_kwargs']\n",
    "\n",
    "    for dataset in senteval_datasets:\n",
    "        print(f'Running: {loss_name} on {dataset}')\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for loop_count in range(0, total_runs):\n",
    "            # Dataset Preparation...\n",
    "            train_dataset, test_dataset = prepare_dataset(dataset)\n",
    "\n",
    "            # Model Preparation...\n",
    "            model, tokenizer = get_model_tokenizer(model_id, loss_type)\n",
    "\n",
    "            # Tokenize Batch...\n",
    "            train_loader, test_loader = tokenize_dataset_batch(train_dataset, test_dataset, tokenizer, batch_size=batch_size)\n",
    "\n",
    "            # Training Loop...\n",
    "            if loss_name != 'without_ft':\n",
    "                model = train(model, train_loader, loss_type, epochs=5, loss_name=loss_name, **loss_kwargs)\n",
    "\n",
    "            # Evaluation loop...\n",
    "            if loss_type == 'clf':\n",
    "                accuracy = evaluate_clf(model, test_loader)\n",
    "            else:\n",
    "                accuracy = evaluate_emb(model, train_loader, test_loader)\n",
    "            # print(f'Loop {loop_count} Accuracy - {accuracy}')\n",
    "            total_accuracy += accuracy\n",
    "        accuracy_list.append({'loss': loss_name, 'dataset': dataset, 'accuracy': total_accuracy / total_runs})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x_Ks8ugG8cot",
    "-3Su9f0u8coz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
